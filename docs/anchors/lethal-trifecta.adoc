= Lethal Trifecta
:categories: software-architecture
:roles: software-architect, software-developer, devops-engineer, consultant, team-lead
:related: regulated-environment
:proponents: Joy Heron
:tags: security, ai-agents, prompt-injection, agentic-ai, mcp

[%collapsible]
====
[discrete]
== *Core Concepts*:

Three risk dimensions:: An AI agent/assistant becomes a critical security risk when it simultaneously combines all three of the following:

Access to private data:: The agent can read sensitive, personal, or confidential information (files, emails, code, credentials)

External communication:: The agent can send information outward — via internet access, APIs, email, or other channels

Handling of untrusted content:: The agent processes content from external or uncontrolled sources (web pages, emails, documents) that may contain prompt injection attacks

Combinatorial danger:: Each dimension alone is manageable; the simultaneous presence of all three enables attackers to exfiltrate private data or trigger unwanted actions by injecting malicious instructions into untrusted content

Agents Rule of Two:: A heuristic derived from the trifecta: a function/capability should combine at most two of the three risk dimensions; any capability combining all three (e.g., reading and sending email) should not be deployed

Growing attack surface:: Every new skill added to an AI agent potentially expands the set of private data exposed, adds communication channels, and introduces new sources of untrusted content — enlarging the trifecta risk with each addition

Sandboxing as partial mitigation:: Physical or OS-level sandboxing limits the blast radius but does not eliminate the trifecta as long as internet access and untrusted content remain


Key Proponent:: Joy Heron (INNOQ, "Unsichere KI-Assistenten dürfen nicht zur Normalität werden", February 2026)

[discrete]
== *When to Use*:

* Evaluating whether an AI agent or assistant is safe to deploy
* Assessing the risk of adding a new capability/skill to an existing AI agent
* Designing safer AI agent architectures by isolating capabilities
* Reviewing MCP server configurations or agentic tool integrations
* Deciding whether to adopt an AI assistant product or platform

[discrete]
== *Related Anchors*:

* <<regulated-environment,Regulated Environment>>
====
